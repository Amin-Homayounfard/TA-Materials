\documentclass{article}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage[top=1cm]{geometry}
\title{Mathematical Statistics I\\ Recitation Session 5}
\date{}

\newtheorem{theorem}{Theorem}
\newtheorem*{theorem*}{Theorem}

\theoremstyle{definition}

\newtheorem{definition}{Definition}
\newtheorem*{definition*}{Definition}

\newtheorem{exercise}{Exercise}
\newtheorem*{exercise*}{Exercise}
\begin{document}
	\maketitle
	\begin{definition*}
		Suppose that random variables $X_1, \ldots, X_n$ have a joint density or frequency function $f\left(x_1, x_2, \ldots, x_n; \theta\right)$. Given observed values $X_i=x_i$, where $i=1, \ldots, n$, the likelihood of $\theta$ as a function of $x_1, x_2, \ldots, x_n$ is defined as
		$$
		L(\theta;x_1, x_2, \ldots, x_n)=f\left(x_1, x_2, \ldots, x_n; \theta\right)
		$$
		Note that we consider the joint density as a function of $\theta$ rather than as a function of the $x_i$.
	\end{definition*}
		
	\begin{definition*}
		For each sample point $\mathbf{x}$, let $\hat{\theta}(\mathbf{x})$ be a parameter value at which $L(\theta; \mathbf{x})$ attains its maximum as a function of $\theta$, with $\mathbf{x}$ held fixed. A maximum likelihood estimator (MLE) of the parameter $\theta$ based on a sample $\mathbf{X}$ is $\hat{\theta}(\mathbf{X})$.
	\end{definition*}
	
	\begin{theorem*}
		(Invariance property of MLEs) If $\hat{\theta}$ is the MLE of $\theta$, then for any function $\tau(\theta)$, the MLE of $\tau(\theta)$ is $\tau(\hat{\theta})$.
	\end{theorem*}
	
	
	\begin{exercise*}
		Let $X_1, \ldots, X_n$ be iid with one of two pdfs. If $\theta=0$, then
		$$
		f(x;\theta)= \begin{cases}1 & \text { if } 0<x<1 \\ 0 & \text { otherwise }\end{cases}
		$$
		while if $\theta=1$, then
		$$
		f(x;\theta)= \begin{cases}1 /(2 \sqrt{x}) & \text { if } 0<x<1 \\ 0 & \text { otherwise }\end{cases}
		$$
		Find the MLE of $\theta$.
	\end{exercise*}
	 
	\begin{exercise*}
		Let $X_1, \ldots, X_n$ be iid with pdf
		$$
		f(x;\theta)=\frac{1}{\theta}, \quad 0 \leq x \leq \theta, \quad \theta>0 .
		$$
		Estimate $\theta$ using both the method of moments and maximum likelihood. Calculate the means and variances of the two estimators. Which one should be preferred and why?
	\end{exercise*}
	
	\begin{exercise*}
		The independent random variables $X_1, \ldots, X_n$ have the common distribution
		$$
		F\left(x ; \alpha, \beta\right)= \begin{cases}0 & \text { if } x<0 \\ (x / \beta)^\alpha & \text { if } 0 \leq x \leq \beta \\ 1 & \text { if } x>\beta,\end{cases}
		$$
		where the parameters $\alpha$ and $\beta$ are positive.\\
		(a) Find a two-dimensional sufficient statistic for $(\alpha, \beta)$.\\
		(b) Find the MLEs of $\alpha$ and $\beta$.
	\end{exercise*}
	
	\begin{exercise*}
		Let $X=\left(X_1, \ldots, X_n\right)$ be a random sample of random variables with probability density $f_\theta$. Find an MLE (maximum likelihood estimator) of $\theta$ in each of the following cases.\\
		(i) $f_\theta(x)=\theta^{-1} I_{\{1, \ldots, \theta\}}(x), \theta$ is an integer between 1 and $\theta_0$.\\
		(ii) $f_\theta(x)=e^{-(x-\theta)} I_{(\theta, \infty)}(x), \theta>0$.\\
		(iii) $f_\theta(x)=\theta(1-x)^{\theta-1} I_{(0,1)}(x), \theta>1$.\\
		(iv) $f_\theta(x)=\frac{\theta}{1-\theta} x^{(2 \theta-1) /(1-\theta)} I_{(0,1)}(x), \theta \in\left(\frac{1}{2}, 1\right)$.\\
		(v) $f_\theta(x)=\theta^x(1-\theta)^{1-x} I_{\{0,1\}}(x), \theta \in\left[\frac{1}{2}, \frac{3}{4}\right]$.
	\end{exercise*}
	\pagenumbering{gobble}
\end{document}