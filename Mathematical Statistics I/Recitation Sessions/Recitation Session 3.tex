\documentclass{article}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage[margin=3.5cm]{geometry}
\title{Mathematical Statistics I\\ Recitation Session 3}
\date{}

\newtheorem{theorem}{Theorem}
\newtheorem*{theorem*}{Theorem}

\theoremstyle{definition}

\newtheorem{definition}{Definition}
\newtheorem*{definition*}{Definition}

\newtheorem{exercise}{Exercise}
\newtheorem*{exercise*}{Exercise}
\begin{document}
	\maketitle
	\begin{theorem*}
		Let $X_1, \ldots, X_n$ be iid observations from a pdf or pmf $f(x;\boldsymbol{\theta})$ that belongs to an exponential family given by
		$$
		f(x;\boldsymbol{\theta})=h(x) c(\theta) \exp \left(\sum_{i=1}^k w_i(\boldsymbol{\theta}) t_i(x)\right),
		$$
		where $\boldsymbol{\theta}=\left(\theta_1, \theta_2, \ldots, \theta_d\right), d \leq k$. Then
		$$
		T(\mathbf{X})=\left(\sum_{j=1}^n t_1\left(X_j\right), \ldots, \sum_{j=1}^n t_k\left(X_j\right)\right)
		$$
		is a sufficient statistic for $\boldsymbol{\theta}$.
	\end{theorem*}
	\begin{definition*}
	A sufficient statistic $T(\mathbf{X})$ is called a minimal sufficient statistic if, for any other sufficient statistic $T^{\prime}(\mathbf{X}), T(\mathbf{x})$ is a function of $T^{\prime}(\mathbf{x})$.
	\end{definition*}
	
	\begin{theorem*}
		Let $f(\mathbf{x};\theta)$ be the pmf or pdf of a sample $\mathbf{X}$. Suppose there exists a function $T(\mathbf{x})$ such that, for every two sample points $\mathbf{x}$ and $\mathbf{y}$, the ratio $f(\mathbf{x} ;\theta) / f(\mathbf{y};\theta)$ is constant as a function of $\theta$ if and only if $T(\mathbf{x})=T(\mathbf{y})$. Then $T(\mathbf{X})$ is a minimal sufficient statistic for $\theta$.
	\end{theorem*}

	
	\begin{definition*}
		 Let $f(t;\theta)$ be a family of pdfs or pmfs for a statistic $T(\mathbf{X})$. The family of probability distributions is called complete if $\mathrm{E}_\theta \left(g(T)\right)=0$ for all $\theta$ implies $P_\theta(g(T)=0)=1$ for all $\theta$. Equivalently, $T(\mathbf{X})$ is called a complete statistic.
	\end{definition*}
	
	\begin{theorem*}
		Let $X_1, \ldots, X_n$ be iid observations from an exponential family with pdf or pmf of the form
		$$
		f(x;\boldsymbol{\theta})=h(x) c(\boldsymbol{\theta}) \exp \left(\sum_{j=1}^k w\left(\theta_j\right) t_j(x)\right),
		$$
		where $\boldsymbol{\theta}=\left(\theta_1, \theta_2, \ldots, \theta_k\right)$. Then the statistic
		$$
		T(\mathbf{X})=\left(\sum_{i=1}^n t_1\left(X_i\right), \sum_{i=1}^n t_2\left(X_i\right), \ldots, \sum_{i=1}^n t_k\left(X_i\right)\right)
		$$
		is complete as long as the parameter space $\Theta$ contains an open set in $\Re^k$.
	\end{theorem*}
	\begin{definition*}
		A statistic $S(X)$ whose distribution does not depend on the parameter $\theta$ is called an ancillary statistic.
		
	\end{definition*}
	\begin{exercise*}
		Let $X_1, \ldots, X_n$ be independent random variables with densities
		$$
		f_{X_i}(x; \theta)=\left\{\begin{array}{ll}
			e^{i \theta-x} & x \geq i \theta \\
			0 & x<i \theta .
		\end{array} \right.
		$$
		Prove that $T=\min _i\left(X_i / i\right)$ is a sufficient statistic for $\theta$.
	\end{exercise*}
	\begin{exercise*}
		Let $X_1, \ldots, X_n$ be independent random variables with pdfs
		$$
		f\left(x_i ; \theta\right)= \begin{cases}\frac{1}{2 i \theta} & -i(\theta-1)<x_i<i(\theta+1) \\ 0 & \text { otherwise},\end{cases}
		$$
		where $\theta>0$. Find a two-dimensional sufficient statistic for $\theta$.
	\end{exercise*}
	\begin{exercise*}
		Suppose $X_1, \ldots, X_n$ are iid uniform observations on the interval $(0, \theta),0<\theta<\infty$. Show that $T(\mathbf{X})=X_{(n)}$ is a complete statistic.
	\end{exercise*}
		\begin{exercise*}
		Let $X_1, \ldots, X_n$ be i.i.d. from the $N\left(\theta, \theta^2\right)$ distribution, where $\theta>0$ is a parameter. Find a minimal sufficient statistic for $\theta$ and show whether it is complete.
	\end{exercise*}
	
	\begin{exercise*}
		Suppose that $\left(X_1, Y_1\right), \ldots,\left(X_n, Y_n\right)$ are i.i.d. random 2 -vectors having the normal distribution with $E\left(X_1\right)=E\left(Y_1\right)=0, \operatorname{Var}\left(X_1\right)=\operatorname{Var}\left(Y_1\right)=$ 1 , and $\operatorname{Cov}\left(X_1, Y_1\right)=\theta \in(-1,1)$.\\
		(a) Find a minimal sufficient statistic for $\theta$.\\
		(b) Show whether the minimal sufficient statistic in (a) is complete or not.
	\end{exercise*}
	\begin{exercise*}
		For each of the following pdfs let $X_1, \ldots, X_n$ be iid sample. Find a complete sufficient statistic, or show that one does not exist.\\
		(a) $f(x;\theta)=\frac{\theta}{(1+x)^{1+\theta}}, \quad 0<x<\infty, \quad \theta>0$\\
		(b) $f(x;\theta)=\frac{(\log \theta) \theta^x}{\theta-1}, \quad 0<x<1, \quad \theta>1$\\
	\end{exercise*}
	\begin{exercise*}
		Let $X_1, \ldots, X_n$ be a random sample from the inverse Gaussian distribution with pdf
		$$
		f(x;\mu, \lambda)=\left(\frac{\lambda}{2 \pi x^3}\right)^{1 / 2} e^{\frac{-\lambda(x-\mu)^2}{2 \mu^2 x}}, \quad 0<x<\infty .
		$$
		Show that the statistics
		$$
		\bar{X}=\frac{1}{n} \sum_{i=1}^n X_i \quad \text { and } \quad T=\frac{n}{\sum_{i=1}^n \frac{1}{X_i}-\frac{1}{\bar{X}}}
		$$
		are sufficient and complete.
	\end{exercise*}

	\pagenumbering{gobble}
\end{document}